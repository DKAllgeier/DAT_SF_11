{
 "metadata": {
  "name": "",
  "signature": "sha256:fe7e312a37d1351c99337596ab9b698f5f76d09adc29821082067715f1ba66d8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using tf-idf and random forests"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Newsgroups Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will take a look at some of the twenty newsgroups dataset, another common dataset for classification. Note that the data is fetched from."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "\n",
      "# We will use four of the twenty newsgroups\n",
      "categories = ['alt.atheism',\n",
      "              'talk.religion.misc',\n",
      "              'comp.graphics',\n",
      "              'sci.space']\n",
      "\n",
      "twenty_train_subset = fetch_20newsgroups(subset='train', categories=categories) #.target\n",
      "twenty_test_subset = fetch_20newsgroups(subset='test', categories=categories)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:sklearn.datasets.twenty_newsgroups:Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have lists of messages (as strings) in the `.data` members."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "datasets.get_data_home()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "'/Users/danielallgeier/scikit_learn_data'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Features from text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some ways to generate features from the text:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Count Vectorizer\n",
      "Count Vectorizer is the easiest text processing utility to understand, it simply counts the occurances of non-stopwords. (what is a stopword?)<br>\n",
      "First, let's check out our data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print twenty_train_subset.data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "From: rych@festival.ed.ac.uk (R Hawkes)\n",
        "Subject: 3DS: Where did all the texture rules go?\n",
        "Lines: 21\n",
        "\n",
        "Hi,\n",
        "\n",
        "I've noticed that if you only save a model (with all your mapping planes\n",
        "positioned carefully) to a .3DS file that when you reload it after restarting\n",
        "3DS, they are given a default position and orientation.  But if you save\n",
        "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
        "know why this information is not stored in the .3DS file?  Nothing is\n",
        "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
        "I'd like to be able to read the texture rule information, does anyone have \n",
        "the format for the .PRJ file?\n",
        "\n",
        "Is the .CEL file format available from somewhere?\n",
        "\n",
        "Rych\n",
        "\n",
        "======================================================================\n",
        "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
        "Virtual Environment Laboratory\n",
        "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
        "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
        "======================================================================\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vectorizer = CountVectorizer(stop_words='english') #you can also just pass specific words you choose"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "one_message = vectorizer.fit_transform(twenty_train_subset.data[0:3]).todense().tolist()[0]\n",
      "zip(vectorizer.get_feature_names(),one_message)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[(u'0150', 1L),\n",
        " (u'02', 0L),\n",
        " (u'020359', 0L),\n",
        " (u'04', 0L),\n",
        " (u'07', 0L),\n",
        " (u'08', 0L),\n",
        " (u'15', 0L),\n",
        " (u'19', 0L),\n",
        " (u'1970', 0L),\n",
        " (u'1993', 0L),\n",
        " (u'1993apr19', 0L),\n",
        " (u'20', 0L),\n",
        " (u'20apr199301460499', 0L),\n",
        " (u'21', 1L),\n",
        " (u'23', 0L),\n",
        " (u'2400x4', 0L),\n",
        " (u'245', 0L),\n",
        " (u'26996', 0L),\n",
        " (u'31', 2L),\n",
        " (u'3205', 0L),\n",
        " (u'3426', 1L),\n",
        " (u'3ds', 4L),\n",
        " (u'4366', 0L),\n",
        " (u'44', 2L),\n",
        " (u'55', 0L),\n",
        " (u'604', 0L),\n",
        " (u'650', 1L),\n",
        " (u'667', 1L),\n",
        " (u'_perijoves_', 0L),\n",
        " (u'able', 1L),\n",
        " (u'ac', 2L),\n",
        " (u'access', 0L),\n",
        " (u'actually', 0L),\n",
        " (u'almanac', 0L),\n",
        " (u'apoapsis', 0L),\n",
        " (u'apr', 0L),\n",
        " (u'article', 0L),\n",
        " (u'available', 1L),\n",
        " (u'b645zaw', 0L),\n",
        " (u'backing', 0L),\n",
        " (u'barring', 0L),\n",
        " (u'bc', 0L),\n",
        " (u'biblical', 0L),\n",
        " (u'brader', 0L),\n",
        " (u'british', 0L),\n",
        " (u'bunch', 0L),\n",
        " (u'ca', 0L),\n",
        " (u'canada', 0L),\n",
        " (u'carefully', 1L),\n",
        " (u'carried', 0L),\n",
        " (u'cel', 1L),\n",
        " (u'central', 0L),\n",
        " (u'centuries', 0L),\n",
        " (u'children', 0L),\n",
        " (u'circa', 0L),\n",
        " (u'cites', 0L),\n",
        " (u'columbia', 0L),\n",
        " (u'com', 0L),\n",
        " (u'comet', 0L),\n",
        " (u'contrary', 0L),\n",
        " (u'corruption', 0L),\n",
        " (u'couldn', 0L),\n",
        " (u'default', 1L),\n",
        " (u'delusional', 0L),\n",
        " (u'demonstrating', 0L),\n",
        " (u'dept', 1L),\n",
        " (u'deranged', 0L),\n",
        " (u'did', 1L),\n",
        " (u'does', 2L),\n",
        " (u'ed', 2L),\n",
        " (u'edinburgh', 1L),\n",
        " (u'edu', 0L),\n",
        " (u'email', 1L),\n",
        " (u'enclosed', 0L),\n",
        " (u'environment', 1L),\n",
        " (u'evidence', 0L),\n",
        " (u'evil', 0L),\n",
        " (u'explicitly', 1L),\n",
        " (u'f208', 0L),\n",
        " (u'fanatic', 0L),\n",
        " (u'fax', 1L),\n",
        " (u'feb', 0L),\n",
        " (u'festival', 2L),\n",
        " (u'fidonet', 0L),\n",
        " (u'figure', 0L),\n",
        " (u'file', 6L),\n",
        " (u'fisher', 0L),\n",
        " (u'folks', 0L),\n",
        " (u'format', 2L),\n",
        " (u'frog', 0L),\n",
        " (u'fruitcakes', 0L),\n",
        " (u'given', 1L),\n",
        " (u'got', 0L),\n",
        " (u'gotten', 0L),\n",
        " (u'gt', 0L),\n",
        " (u'hawkes', 2L),\n",
        " (u'hi', 1L),\n",
        " (u'hisse', 0L),\n",
        " (u'holocaust', 0L),\n",
        " (u'home', 0L),\n",
        " (u'information', 2L),\n",
        " (u'internet', 0L),\n",
        " (u'island', 0L),\n",
        " (u'jg', 0L),\n",
        " (u'jgarland', 0L),\n",
        " (u'jim', 0L),\n",
        " (u'jones', 0L),\n",
        " (u'jupiter', 0L),\n",
        " (u'just', 0L),\n",
        " (u'kean', 0L),\n",
        " (u'ken', 0L),\n",
        " (u'killed', 0L),\n",
        " (u'kmcvay', 0L),\n",
        " (u'know', 1L),\n",
        " (u'koresh', 0L),\n",
        " (u'laboratory', 1L),\n",
        " (u'ladysmith', 0L),\n",
        " (u'language', 0L),\n",
        " (u'learned', 0L),\n",
        " (u'like', 1L),\n",
        " (u'lines', 1L),\n",
        " (u'lot', 0L),\n",
        " (u'mail', 0L),\n",
        " (u'mania', 0L),\n",
        " (u'manual', 1L),\n",
        " (u'mapping', 1L),\n",
        " (u'mark', 0L),\n",
        " (u'mb', 0L),\n",
        " (u'mcvay', 0L),\n",
        " (u'mean', 0L),\n",
        " (u'message', 0L),\n",
        " (u'messenger', 0L),\n",
        " (u'model', 1L),\n",
        " (u'msb', 0L),\n",
        " (u'msged', 0L),\n",
        " (u'mun', 0L),\n",
        " (u'n103', 0L),\n",
        " (u'neccessary', 0L),\n",
        " (u'newtout', 0L),\n",
        " (u'nope', 0L),\n",
        " (u'noticed', 1L),\n",
        " (u'old', 0L),\n",
        " (u'oneb', 0L),\n",
        " (u'orbit', 0L),\n",
        " (u'org', 0L),\n",
        " (u'organization', 0L),\n",
        " (u'orientation', 2L),\n",
        " (u'p201', 0L),\n",
        " (u'perew', 0L),\n",
        " (u'periapsis', 0L),\n",
        " (u'perijove', 0L),\n",
        " (u'planes', 1L),\n",
        " (u'position', 1L),\n",
        " (u'positioned', 1L),\n",
        " (u'positions', 1L),\n",
        " (u'preserved', 1L),\n",
        " (u'prj', 3L),\n",
        " (u'psychology', 1L),\n",
        " (u'public', 0L),\n",
        " (u'read', 1L),\n",
        " (u'reload', 1L),\n",
        " (u'restarting', 1L),\n",
        " (u'rotten', 0L),\n",
        " (u'rule', 1L),\n",
        " (u'rules', 2L),\n",
        " (u'rych', 3L),\n",
        " (u'rycharde', 1L),\n",
        " (u'ryugen', 0L),\n",
        " (u'said', 1L),\n",
        " (u'salute', 0L),\n",
        " (u'satisfy', 0L),\n",
        " (u'save', 2L),\n",
        " (u'saving', 1L),\n",
        " (u'say', 0L),\n",
        " (u'says', 0L),\n",
        " (u'sco', 0L),\n",
        " (u'sender', 0L),\n",
        " (u'serving', 0L),\n",
        " (u'simply', 0L),\n",
        " (u'sorry', 0L),\n",
        " (u'sq', 0L),\n",
        " (u'stephen', 0L),\n",
        " (u'stored', 1L),\n",
        " (u'subject', 1L),\n",
        " (u'sure', 0L),\n",
        " (u'surprised', 0L),\n",
        " (u'talking', 0L),\n",
        " (u'tape', 0L),\n",
        " (u'tel', 1L),\n",
        " (u'temporary', 0L),\n",
        " (u'texture', 3L),\n",
        " (u'things', 0L),\n",
        " (u'thought', 0L),\n",
        " (u'time', 0L),\n",
        " (u'ucs', 0L),\n",
        " (u'uk', 2L),\n",
        " (u'univ', 1L),\n",
        " (u'unlikely', 0L),\n",
        " (u'used', 0L),\n",
        " (u'usenet', 0L),\n",
        " (u'uta', 0L),\n",
        " (u'utarlg', 0L),\n",
        " (u'v32', 0L),\n",
        " (u'vancouver', 0L),\n",
        " (u've', 1L),\n",
        " (u'virtual', 1L),\n",
        " (u'writes', 0L),\n",
        " (u'xenix', 0L),\n",
        " (u'z1', 0L)]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This makes a matrix of word counts, where each row is a document and each column is the word, the cell matrix[document, word] contains the count of word in document.\n",
      "<br><br>\n",
      "Now try this with the whole training subset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "<2034x33815 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 233470 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default this returns a sparse matrix, which will save memory.\n",
      "\n",
      "Also, notice our stop words:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_stop_words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "frozenset({'a',\n",
        "           'about',\n",
        "           'above',\n",
        "           'across',\n",
        "           'after',\n",
        "           'afterwards',\n",
        "           'again',\n",
        "           'against',\n",
        "           'all',\n",
        "           'almost',\n",
        "           'alone',\n",
        "           'along',\n",
        "           'already',\n",
        "           'also',\n",
        "           'although',\n",
        "           'always',\n",
        "           'am',\n",
        "           'among',\n",
        "           'amongst',\n",
        "           'amoungst',\n",
        "           'amount',\n",
        "           'an',\n",
        "           'and',\n",
        "           'another',\n",
        "           'any',\n",
        "           'anyhow',\n",
        "           'anyone',\n",
        "           'anything',\n",
        "           'anyway',\n",
        "           'anywhere',\n",
        "           'are',\n",
        "           'around',\n",
        "           'as',\n",
        "           'at',\n",
        "           'back',\n",
        "           'be',\n",
        "           'became',\n",
        "           'because',\n",
        "           'become',\n",
        "           'becomes',\n",
        "           'becoming',\n",
        "           'been',\n",
        "           'before',\n",
        "           'beforehand',\n",
        "           'behind',\n",
        "           'being',\n",
        "           'below',\n",
        "           'beside',\n",
        "           'besides',\n",
        "           'between',\n",
        "           'beyond',\n",
        "           'bill',\n",
        "           'both',\n",
        "           'bottom',\n",
        "           'but',\n",
        "           'by',\n",
        "           'call',\n",
        "           'can',\n",
        "           'cannot',\n",
        "           'cant',\n",
        "           'co',\n",
        "           'con',\n",
        "           'could',\n",
        "           'couldnt',\n",
        "           'cry',\n",
        "           'de',\n",
        "           'describe',\n",
        "           'detail',\n",
        "           'do',\n",
        "           'done',\n",
        "           'down',\n",
        "           'due',\n",
        "           'during',\n",
        "           'each',\n",
        "           'eg',\n",
        "           'eight',\n",
        "           'either',\n",
        "           'eleven',\n",
        "           'else',\n",
        "           'elsewhere',\n",
        "           'empty',\n",
        "           'enough',\n",
        "           'etc',\n",
        "           'even',\n",
        "           'ever',\n",
        "           'every',\n",
        "           'everyone',\n",
        "           'everything',\n",
        "           'everywhere',\n",
        "           'except',\n",
        "           'few',\n",
        "           'fifteen',\n",
        "           'fify',\n",
        "           'fill',\n",
        "           'find',\n",
        "           'fire',\n",
        "           'first',\n",
        "           'five',\n",
        "           'for',\n",
        "           'former',\n",
        "           'formerly',\n",
        "           'forty',\n",
        "           'found',\n",
        "           'four',\n",
        "           'from',\n",
        "           'front',\n",
        "           'full',\n",
        "           'further',\n",
        "           'get',\n",
        "           'give',\n",
        "           'go',\n",
        "           'had',\n",
        "           'has',\n",
        "           'hasnt',\n",
        "           'have',\n",
        "           'he',\n",
        "           'hence',\n",
        "           'her',\n",
        "           'here',\n",
        "           'hereafter',\n",
        "           'hereby',\n",
        "           'herein',\n",
        "           'hereupon',\n",
        "           'hers',\n",
        "           'herself',\n",
        "           'him',\n",
        "           'himself',\n",
        "           'his',\n",
        "           'how',\n",
        "           'however',\n",
        "           'hundred',\n",
        "           'i',\n",
        "           'ie',\n",
        "           'if',\n",
        "           'in',\n",
        "           'inc',\n",
        "           'indeed',\n",
        "           'interest',\n",
        "           'into',\n",
        "           'is',\n",
        "           'it',\n",
        "           'its',\n",
        "           'itself',\n",
        "           'keep',\n",
        "           'last',\n",
        "           'latter',\n",
        "           'latterly',\n",
        "           'least',\n",
        "           'less',\n",
        "           'ltd',\n",
        "           'made',\n",
        "           'many',\n",
        "           'may',\n",
        "           'me',\n",
        "           'meanwhile',\n",
        "           'might',\n",
        "           'mill',\n",
        "           'mine',\n",
        "           'more',\n",
        "           'moreover',\n",
        "           'most',\n",
        "           'mostly',\n",
        "           'move',\n",
        "           'much',\n",
        "           'must',\n",
        "           'my',\n",
        "           'myself',\n",
        "           'name',\n",
        "           'namely',\n",
        "           'neither',\n",
        "           'never',\n",
        "           'nevertheless',\n",
        "           'next',\n",
        "           'nine',\n",
        "           'no',\n",
        "           'nobody',\n",
        "           'none',\n",
        "           'noone',\n",
        "           'nor',\n",
        "           'not',\n",
        "           'nothing',\n",
        "           'now',\n",
        "           'nowhere',\n",
        "           'of',\n",
        "           'off',\n",
        "           'often',\n",
        "           'on',\n",
        "           'once',\n",
        "           'one',\n",
        "           'only',\n",
        "           'onto',\n",
        "           'or',\n",
        "           'other',\n",
        "           'others',\n",
        "           'otherwise',\n",
        "           'our',\n",
        "           'ours',\n",
        "           'ourselves',\n",
        "           'out',\n",
        "           'over',\n",
        "           'own',\n",
        "           'part',\n",
        "           'per',\n",
        "           'perhaps',\n",
        "           'please',\n",
        "           'put',\n",
        "           'rather',\n",
        "           're',\n",
        "           'same',\n",
        "           'see',\n",
        "           'seem',\n",
        "           'seemed',\n",
        "           'seeming',\n",
        "           'seems',\n",
        "           'serious',\n",
        "           'several',\n",
        "           'she',\n",
        "           'should',\n",
        "           'show',\n",
        "           'side',\n",
        "           'since',\n",
        "           'sincere',\n",
        "           'six',\n",
        "           'sixty',\n",
        "           'so',\n",
        "           'some',\n",
        "           'somehow',\n",
        "           'someone',\n",
        "           'something',\n",
        "           'sometime',\n",
        "           'sometimes',\n",
        "           'somewhere',\n",
        "           'still',\n",
        "           'such',\n",
        "           'system',\n",
        "           'take',\n",
        "           'ten',\n",
        "           'than',\n",
        "           'that',\n",
        "           'the',\n",
        "           'their',\n",
        "           'them',\n",
        "           'themselves',\n",
        "           'then',\n",
        "           'thence',\n",
        "           'there',\n",
        "           'thereafter',\n",
        "           'thereby',\n",
        "           'therefore',\n",
        "           'therein',\n",
        "           'thereupon',\n",
        "           'these',\n",
        "           'they',\n",
        "           'thick',\n",
        "           'thin',\n",
        "           'third',\n",
        "           'this',\n",
        "           'those',\n",
        "           'though',\n",
        "           'three',\n",
        "           'through',\n",
        "           'throughout',\n",
        "           'thru',\n",
        "           'thus',\n",
        "           'to',\n",
        "           'together',\n",
        "           'too',\n",
        "           'top',\n",
        "           'toward',\n",
        "           'towards',\n",
        "           'twelve',\n",
        "           'twenty',\n",
        "           'two',\n",
        "           'un',\n",
        "           'under',\n",
        "           'until',\n",
        "           'up',\n",
        "           'upon',\n",
        "           'us',\n",
        "           'very',\n",
        "           'via',\n",
        "           'was',\n",
        "           'we',\n",
        "           'well',\n",
        "           'were',\n",
        "           'what',\n",
        "           'whatever',\n",
        "           'when',\n",
        "           'whence',\n",
        "           'whenever',\n",
        "           'where',\n",
        "           'whereafter',\n",
        "           'whereas',\n",
        "           'whereby',\n",
        "           'wherein',\n",
        "           'whereupon',\n",
        "           'wherever',\n",
        "           'whether',\n",
        "           'which',\n",
        "           'while',\n",
        "           'whither',\n",
        "           'who',\n",
        "           'whoever',\n",
        "           'whole',\n",
        "           'whom',\n",
        "           'whose',\n",
        "           'why',\n",
        "           'will',\n",
        "           'with',\n",
        "           'within',\n",
        "           'without',\n",
        "           'would',\n",
        "           'yet',\n",
        "           'you',\n",
        "           'your',\n",
        "           'yours',\n",
        "           'yourself',\n",
        "           'yourselves'})"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###n-gram\n",
      "The basic idea of n-gram is to take a sequence of objects and make sense of it. Take, for example:<br><br>\n",
      "\n",
      "`I am Sam,`<br>\n",
      "`Sam I am,`<br>\n",
      "`Do you like green eggs and ham?`<br><br>\n",
      "\n",
      "To gram this we will extract all sequences of length `n` like so (for `n=3`):<br><br>\n",
      "`(i,am,sam),(am,sam,sam),(sam,sam,i),(sam,i,am),...`<br><br>\n",
      "\n",
      "scikit gives us the option to use n-grams as features their extraction module:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Include every 1-gram, 2-gram, and 3-gram\n",
      "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that this heavily inflates feature set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<2034x688611 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 1437560 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###TF-IDF"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Additionally, we could use a tf-idf representation, which stands for Term Frequency, Inverse Document Frequency.\n",
      "\n",
      "This value is the product of two intermediate values, the Term Frequency and the Inverse Document Frequency.\n",
      "\n",
      "The Term Frequency is equivalent to the `CountVectorizer` features, the number of times or count that a word appear in the document. This is our most basic representation of text.\n",
      "\n",
      "To establish Inverse Document Frequency, first let's define Document Frequency. This is the percentage of documents that a particular word appears in. For example, the word `the` might appear in 100% of documents, while words like `Syria` would likely have low document frequency. Inverse Document Frequency is simply 1 / Document Frequency (although often the log is also taken).\n",
      "\n",
      "Let, $D$ be the set of all documents:\n",
      "$$\n",
      "idf(word,D) = \\log \\frac{N}{|\\{d \\in D : t \\in d\\}|}\n",
      "$$\n",
      "\n",
      "Notice that this has the neat property that if a word occurs in ALL documents, $idf = 0$ so this naturally controls for stop words \n",
      "\n",
      "So tf-idf is Term Frequency * Inverse Document Frequency, or similar to Term Frequency / Document Frequency. The intuition is that words that have high weight are those that appear a lot in this document and/or appear in very few other documents (somehow unique to this document).\n",
      "\n",
      "Example:\n",
      "Let, $d_1 = $\"i am sam sam i am\", and $d_2 = $ \"i do not like them sam i am\", then:\n",
      "$$\n",
      "tf(like,d_2) = 1\n",
      "$$\n",
      "$$\n",
      "idf(like,D) = \\log \\frac{2}{1} = 0.3010\n",
      "$$\n",
      "$$\n",
      "tfidf(like,d_2) = 1*0.3010\n",
      "$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer()\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 893 ms, sys: 51.5 ms, total: 945 ms\n",
        "Wall time: 945 ms\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can put this together with our other tricks as well...but notice the running time hit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,5))\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 11.4 s, sys: 362 ms, total: 11.8 s\n",
        "Wall time: 11.8 s\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[READ THE DOCS!](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.tree import DecisionTreeClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use predict using our 20-newsgroup dataset above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(stop_words='english')\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "tree_model = DecisionTreeClassifier()\n",
      "print cross_val_score(tree_model, X_train.toarray(), twenty_train_subset.target).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.777252485817\n",
        "CPU times: user 22.5 s, sys: 1.18 s, total: 23.7 s\n",
        "Wall time: 24.1 s\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "rf_model = RandomForestClassifier(n_estimators=20)\n",
      "print cross_val_score(rf_model, X_train.toarray(), twenty_train_subset.target).mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.871704821744\n",
        "CPU times: user 8.15 s, sys: 982 ms, total: 9.13 s\n",
        "Wall time: 9.24 s\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting Important Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This prints the top 10 most important features\n",
      "rf_model.fit(X_train.toarray(),twenty_train_subset.target)\n",
      "sorted(zip(rf_model.feature_importances_, vectorizer.get_feature_names()), reverse=True)[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "[(0.018934349686381087, u'space'),\n",
        " (0.013901247907354771, u'atheists'),\n",
        " (0.012022027380555383, u'christian'),\n",
        " (0.010445973094026418, u'graphics'),\n",
        " (0.0080890208820317243, u'schneider'),\n",
        " (0.0079584037886840358, u'orbit'),\n",
        " (0.0066793553643348224, u'writes'),\n",
        " (0.0064690542837640723, u'year'),\n",
        " (0.0058304152273217199, u'god'),\n",
        " (0.0057123324175749795, u'henry'),\n",
        " (0.0053539665899718196, u'image'),\n",
        " (0.0053366796213793945, u'keith'),\n",
        " (0.0050965726034604584, u'digex'),\n",
        " (0.0050331390170546796, u'launch'),\n",
        " (0.0048488930597269785, u'gov'),\n",
        " (0.0048245089098145994, u'sci'),\n",
        " (0.0046754333504531341, u'atheism'),\n",
        " (0.0045773380913902793, u'article'),\n",
        " (0.0045433538101446458, u'jesus'),\n",
        " (0.004514708504434051, u'religion')]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Your Turn:\n",
      "* Calculate the learning curve for increasing values of `n_estimators`\n",
      "* Plot the confusion matrix\n",
      "* Report recall, precision and f-score for each class using your best estimator"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Standard imports\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import metrics\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(__doc__)\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn import cross_validation\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.datasets import load_digits\n",
      "from sklearn.learning_curve import learning_curve\n",
      "\n",
      "\n",
      "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
      "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
      "    \n",
      "    \n",
      "    \"\"\"\n",
      "    Generate a simple plot of the test and traning learning curve.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
      "        An object of that type which is cloned for each validation.\n",
      "\n",
      "    title : string\n",
      "        Title for the chart.\n",
      "\n",
      "    X : array-like, shape (n_samples, n_features)\n",
      "        Training vector, where n_samples is the number of samples and\n",
      "        n_features is the number of features.\n",
      "\n",
      "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
      "        Target relative to X for classification or regression;\n",
      "        None for unsupervised learning.\n",
      "\n",
      "    ylim : tuple, shape (ymin, ymax), optional\n",
      "        Defines minimum and maximum yvalues plotted.\n",
      "\n",
      "    cv : integer, cross-validation generator, optional\n",
      "        If an integer is passed, it is the number of folds (defaults to 3).\n",
      "        Specific cross-validation objects can be passed, see\n",
      "        sklearn.cross_validation module for the list of possible objects\n",
      "\n",
      "    n_jobs : integer, optional\n",
      "        Number of jobs to run in parallel (default 1).\n",
      "    \"\"\"\n",
      "    \n",
      "    plt.figure()\n",
      "    plt.title(title)\n",
      "    if ylim is not None:\n",
      "        plt.ylim(*ylim)\n",
      "    plt.xlabel(\"Training examples\")\n",
      "    plt.ylabel(\"Score\")\n",
      "    train_sizes, train_scores, test_scores = learning_curve(\n",
      "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
      "    train_scores_mean = np.mean(train_scores, axis=1)\n",
      "    train_scores_std = np.std(train_scores, axis=1)\n",
      "    test_scores_mean = np.mean(test_scores, axis=1)\n",
      "    test_scores_std = np.std(test_scores, axis=1)\n",
      "    plt.grid()\n",
      "\n",
      "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
      "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
      "                     color=\"r\")\n",
      "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
      "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
      "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
      "             label=\"Training score\")\n",
      "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
      "             label=\"Cross-validation score\")\n",
      "\n",
      "    plt.legend(loc=\"best\")\n",
      "    return plt\n",
      "\n",
      "\n",
      "X, y = twenty_train_subset.data, twenty_train_subset.target\n",
      "\n",
      "title='Random Forest Learning Curve'\n",
      "\n",
      "cv = cross_validation.ShuffleSplit(twenty_train_subset.data.shape[0], n_iter=10,\n",
      "                                   test_size=0.2, random_state=0)\n",
      "estimator = RandomForestClassifier()\n",
      "plot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cv = cross_validation.ShuffleSplit(twenty_train_subset.data.shape[0], n_iter=10,\n",
        "                                   test_size=0.2, random_state=0)\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'list' object has no attribute 'shape'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-35-edee6fb4e178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Random Forest Learning Curve'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m cv = cross_validation.ShuffleSplit(twenty_train_subset.data.shape[0], n_iter=10,\n\u001b[0m\u001b[1;32m     80\u001b[0m                                    test_size=0.2, random_state=0)\n\u001b[1;32m     81\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test = vectorizer.fit_transform(twenty_test_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "<1353x26922 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 161516 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}